---
layout: post
title: 集成学习
math: true
toc: true
tags:
- 集成学习
- 机器学习
date: 2022-07-04
---
集成学习分为：Bagging和Boosting。

bagging: 从训练集T中有放回的抽取m个样本，训练m个基学习器，通过投票的方式获得最后的预测值(并联)；

boosting: 根据原始样本集训练一个弱学习器，根据这个弱学习器调整样本的权重，再根据新的样本训练新的弱学习器，重复上述过程，直到满足停止的条件(比如：T个学习器)，将所有弱学习器按照各自的权重叠加起来(串联)。

- 基本学习器：决策树

  - 信息增益，信息增益比和基尼系数；


##### bagging代表模型

- 随机森林

  随机森林是bagging的变异代表，它与bagging相似，但又有所不同，样本集也是有放回抽样，但是对于特征，每个基学习它也是随机不重复抽取d个特征。

  ![](C:\Users\HUAWEI\Desktop\image\randomforest.png)

  - 可以不需要做特征选择；
  - 随机森林不需要剪技；
  - 分类是投票，回归是计算平均值获得结果；

- GBDT

  1. **算法流程**：通过多轮迭代，每次迭代产生一个弱学习器，每个弱学习是在上一轮弱学习的残差基础上进行训练的。对弱学习器一般要求足够简单，并且低方差，高偏差，因为GBDT是通过迭代不断的减少偏差来提升模型的精度。也就是弱学习器一般选择回归树，最终的结果就是所有弱学习器加权求和的结果。

     模型的一般式为：  
     $$
     F_m(\pmb{x}) = \sum_{m=1}^M T(\pmb{x}; \theta_m)
     $$
       
     弱分类器的损失函数为：  
     $$
     \hat{\theta}_m = \mathop{arg}\limits_{\theta_m}min \sum_{i=1}^N L(y_i, F_{m-1}(x_i) + 
     T(x_i, \theta_m)）
     $$
        
     为了损失函数不断的减小，并且下降最快，因此让损失函数函数沿着梯度的方向下降。做法为：利用损失函数的负梯度值做为目标值，利用回归树和残差去拟合这个值。这样每次迭代，GBDT都去拟合负梯度，也就是沿着负梯度方向下降。

  2. **特征选择**：遍历所有特征的的所有切分点，选择让误差最小的特征的切分点；

  3. **GBDT分类**：每一轮迭代同时训练跟分类类别一样的弱学习器，比如三分类，那么就是同时训练三个颗回归树，第一棵回归树表示训练第一类: [1, 0, 0]; 第二棵回归树表示训练第二类: [0, 1, 0]; 以此类推。

  4. **GBDT调参**: 迭代次数过多容易过拟合，过少容易欠拟合；

- **XGBoost**

  1. XGBOOST跟GBDT类似，也是拟合参数，但是xgboost在目标函数上进行了优化，使用二阶泰勒展开让目标函数只依赖于每个数据点的一阶导数和二阶导数，这样好求解其他损失函数；并且加入了树的复杂度(叶子个数得分，正则化)，不易过拟合。
  2. 可以并行，但是并行的是计算特征粒度上的并行；
  3. 加入正则化，控制模型复杂度；
  4. 列抽样和Shrinkage(缩减：eta): 缩减为了减少该棵树影响，让后面的树可以学习到更多东西，以免发生过拟合；
  5. **xgboost调参：** 

- **LightGBM**

  **原理:**结合使用了GOSS和EFB的GBDT算法。GOSS单边梯度采样，排除大部分小梯度的样本，仅用剩余的样本计算信息增益。EFB互斥特征绑定，将不同时取非零值的特征合成一个特征，也就是绑定起来。

  1. 更快的训练效率；
  2. 低内存使用；
  3. 更高的准确率；
  4. 支持并行化学习；
  5. 可处理大规模数据；
  6. 支持直接使用分类特征；

  - **特征选择策略(先离散化再划分)：直方图决策树**

    > 对于连续变量，先确定每个特征需要多少bin(直方图条数)，然后均分，将样本属于哪个bin就是那个bin的值，相当于将连续变量离散化；
    >
    > 注意点：
    >
    > 1. 使用bin代替原始数据相当于正则化；
    > 2. 使用bin相当于默认忽略数据的一些细节，相似的数据归为一个bin;(实际表明，对最终结果的精度影响不大)
    > 3. bin的数量越少，正则化越严重，容易欠拟合；
    >
    > 加速小技巧：一个叶子节点的直方图，可以由父节点的直方图和兄弟节点的直方图做差得到；这该叶子节点就不需要遍历所有数据，只需遍历K个bin的值。(父节点相当于总的数据，兄弟节点相当于某个特征小于某个值，而该叶子节点就相当于该特征大于该值；所以就是总的数据 - 小于那部分数据 = 大于那部分数据)

  - **决策树生成策略：**

    > 对于XGBoost是按层生成决策树，每一层的叶子都生成下一层两个叶子，不区别同一层不同叶子的分裂增益，带来不必要的开销；
    >
    > 而LightGBM是按叶子生成的决策树，每次计算所有叶子当中分裂增益最大的叶子进行分裂，以此类推；分裂次数相同的情况下，Leaf-wise的误差会更低。但是容易生成一个很深的决策树，容易产生过拟合，因此加了一个最大深度限制，也就是带有深度限制的叶子生成策略；

  - **直接支持分类特征**

    > 统计离散特征下所有离散值，并且从高到低排序，特征值出现次数少的直接过滤掉(降低过拟合)，剩余特征值为其构造一个bin容器；
    >
    > 如果bin容器个数小于4, 就是直接遍历计算,  one vs other;
    >
    > 如果bin容器较多的话，就采用many vs many 策略，这里再过滤一遍，只允许子集大的bin参加阀值计算；先每个bin计算一个值(一阶梯度/二阶梯度 + 正则项)，按照该值对bin进行从小到大排序，然后从左到右和从右到左搜索，得到最优阀值。不会搜索所有bin容器，有个限定参数(默认：32)，因此bin中最优划分的阀值的左边或者右边的bin容易作为左边树(many)，其他做为右边树(many)；

  - **支持高效并行**

    > 支持特征并行和数据并行

